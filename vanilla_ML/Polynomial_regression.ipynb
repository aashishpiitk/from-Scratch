{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def sigmoid(x): \n",
    "#    return 1/(1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def computeLogistic(X,y,theta,alpha):\n",
    "#    [m,n]=X.shape\n",
    "#    h=np.dot(X,theta)\n",
    "#    H=sigmoid(h)\n",
    "#    J=(-1/m)*(np.dot(y.T,log(H))+np.dot((1-y).T,log(1-H)))\n",
    "#    grad=np.multiply((alpha/m),(np.dot(X.T,H-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLinear(X,y,theta):\n",
    "    alpha=1\n",
    "    [m,n]=X.shape\n",
    "    H=np.dot(X,theta)\n",
    "    error=H-y\n",
    "    J=(1/(2*m))*sum(np.multiply(error,error))#cost\n",
    "    #rint(J,n,\"\\n\")\n",
    "    grad=np.multiply((alpha/m),(np.dot(X.T,error)))\n",
    "    return [J,grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator(X,degree):\n",
    "    x1=X[:,0]#m *1\n",
    "    x2=X[:,1]#m*1\n",
    "    out=np.ones([X.shape[0],1])#m*n\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(0,i+1):\n",
    "            temp=np.multiply(np.power(x1,i-j),np.power(x2,j))\n",
    "            #print(temp.shape)\n",
    "            #print(out.shape)\n",
    "            out=np.append(out,temp.reshape(-1,1),axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,theta,alpha,num_iterations):\n",
    "    J_history=np.zeros([num_iterations,1])\n",
    "    for i in range(num_iterations):\n",
    "        a=computeLinear(X,y,theta)\n",
    "        J_history=a[0]\n",
    "        grad=a[1]\n",
    "        theta=theta-np.multiply(grad,alpha)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when calling this function keep num_iterations=10,000 and alpha=0.001 and degree=2 or 1 to avoid overflow\n",
    "#also keep the entries of X_train between 1 and 1000,as the function is not optimized and uses simple gradient descent\n",
    "def polynomial_regression(X_train,y,theta,alpha,degree,X_test,num_iterations):\n",
    "    X=X_train\n",
    "    [m,n]=X.shape\n",
    "    X_mod=feature_generator(X,degree)#m*some number\n",
    "    theta=np.random.randn(X_mod.shape[1],1)\n",
    "    theta=gradient_descent(X_mod,y,theta,alpha,num_iterations)#n+1 * 1\n",
    "    #temp=np.ones([X.shape[0],1])\n",
    "    X_test=feature_generator(X_test,degree)\n",
    "    #X_test=np.append(temp,X_test,axis=1)\n",
    "    predict=np.dot(X_test,theta)\n",
    "    return predict,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=np.array([[1,2,3],[3,4,5]])\n",
    "#[m,n]=a.shape\n",
    "#a.T\n",
    "#b=[[10],[11]]\n",
    "#a=np.append(b,a,axis=1)\n",
    "#c=[[2],[3]]\n",
    "#print(a)\n",
    "#np.absolute([-1,-2])\n",
    "#import scipy.optimize as opt\n",
    "#result = opt.fmin_tnc(func=computeLinear, x0=theta, fprime=gradient, args=(X, y))\n",
    "#cost(result[0], X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999994]\n",
      " [2.00000001]]\n"
     ]
    }
   ],
   "source": [
    "#demo example\n",
    "X=np.array([[1,2],[3,4]])\n",
    "y=[[1],[2]]\n",
    "theta=[[0],[0],[0]]\n",
    "print((polynomial_regression(X,y,theta,0.001,2,X,10000))[0])\n",
    "#a=computeLinearReg(X,y,theta,0.1)\n",
    "#print(gradient_descent(X,y,theta,0.1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta,X,y):\n",
    "    return computeLinear(X,y,theta)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ideal way of using this function\n",
    "#when calling this function keep num_iterations=10,000 and alpha=0.001 and degree=2 or 1 to avoid overflow\n",
    "#also keep the entries of X_train between 1 and 1000,as the function is not optimized and uses simple gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
